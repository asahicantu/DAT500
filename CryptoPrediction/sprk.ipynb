{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT-500 Project\n",
    "This notebook is the final project\n",
    "Project description: Create a machine learning algorithm capable of training a big dtaset of parquet files containing cryptocurrency history from Binance, save the models for later usage and evaluation.\n",
    "\n",
    "### Fewer settings to be considered:\n",
    "On remote server check HADOOP_HOME environment variable is properly configured\n",
    "default hadoop host is hdfs://master:9000\n",
    "core-site.xml and look for xml element fs.defaultFS\n",
    "#### List of hadoop remote data files directories:\n",
    "\n",
    "<font color=\"Green\">Files</font>\n",
    "\n",
    "- /Data/Abcnews-date-text.csv\n",
    "- /Data/Binance.csv\n",
    "- /Data/CoinMarketCap.csv\n",
    "- /Data/QuandlData.csv\n",
    "- /Data/cointelegraph_news1-1000_content.csv\n",
    "- /Data/cointelegraph_news1-1000_head.csv\n",
    "\n",
    "<font color=\"Green\">Directories </font>\n",
    "\n",
    "- /Data/Binance_Parquet\n",
    "- /Data/Binance_Parquet/ADA-BNB.parquet\n",
    "- /Data/Binance_Parquet/ADA-BTC.parquet\n",
    "- /Data/Binance_Parquet/ADA-BUSD.parquet\n",
    "\n",
    "\n",
    "etc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: push.default is unset; its implicit value has changed in\n",
      "Git 2.0 from 'matching' to 'simple'. To squelch this message\n",
      "and maintain the traditional behavior, use:\n",
      "\n",
      "  git config --global push.default matching\n",
      "\n",
      "To squelch this message and adopt the new behavior now, use:\n",
      "\n",
      "  git config --global push.default simple\n",
      "\n",
      "When push.default is set to 'matching', git will push local branches\n",
      "to the remote branches that already exist with the same name.\n",
      "\n",
      "Since Git 2.0, Git defaults to the more conservative 'simple'\n",
      "behavior, which only pushes the current branch to the corresponding\n",
      "remote branch that 'git pull' uses to update the current branch.\n",
      "\n",
      "See 'git help config' and search for 'push.default' for further information.\n",
      "(the 'simple' mode was introduced in Git 1.7.11. Use the similar mode\n",
      "'current' instead of 'simple' if you sometimes use older versions of Git)\n",
      "\n",
      "Username for 'https://github.com': ^C\n"
     ]
    }
   ],
   "source": [
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HADOOP_HOME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n",
      "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\r\n",
      "<!--\r\n",
      "  Licensed under the Apache License, Version 2.0 (the \"License\");\r\n",
      "  you may not use this file except in compliance with the License.\r\n",
      "  You may obtain a copy of the License at\r\n",
      "\r\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\r\n",
      "\r\n",
      "  Unless required by applicable law or agreed to in writing, software\r\n",
      "  distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
      "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
      "  See the License for the specific language governing permissions and\r\n",
      "  limitations under the License. See accompanying LICENSE file.\r\n",
      "-->\r\n",
      "\r\n",
      "<!-- Put site-specific property overrides in this file. -->\r\n",
      "\r\n",
      "<configuration>\r\n",
      "  <property>\r\n",
      "    <name>fs.default.name</name>\r\n",
      "    <value>hdfs://master:9000</value>\r\n",
      "  </property>\r\n",
      "</configuration>\r\n"
     ]
    }
   ],
   "source": [
    "!cat /usr/local/hadoop/etc/hadoop/core-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyarrow in /home/ubuntu/.local/lib/python3.8/site-packages (0.16.0)\n",
      "Requirement already satisfied: six>=1.0.0 in /usr/local/lib/python3.8/site-packages (from pyarrow) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.8/site-packages (from pyarrow) (1.18.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting hdfs\n",
      "  Downloading hdfs-2.5.8.tar.gz (41 kB)\n",
      "\u001b[K     |████████████████████████████████| 41 kB 183 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting docopt\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Requirement already satisfied: requests>=2.7.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from hdfs) (2.23.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/site-packages (from hdfs) (1.14.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests>=2.7.0->hdfs) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests>=2.7.0->hdfs) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests>=2.7.0->hdfs) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests>=2.7.0->hdfs) (1.25.9)\n",
      "Building wheels for collected packages: hdfs, docopt\n",
      "  Building wheel for hdfs (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hdfs: filename=hdfs-2.5.8-py3-none-any.whl size=33213 sha256=29f7fe6e9a6dd1782497b9b62ef7e69fe979f98f809d360e2a705765dfbe8fb9\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/43/cb/59/3fdce328ada746ea437798538a9808e4f730135f5a26f137a4\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=2413eaff19cd6f1841bae2aebb779f6d062030b9f17b7b9ba7787581c890bb16\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
      "Successfully built hdfs docopt\n",
      "Installing collected packages: docopt, hdfs\n",
      "Successfully installed docopt-0.6.2 hdfs-2.5.8\n"
     ]
    }
   ],
   "source": [
    "!pip3.8 install pyarrow\n",
    "!pip3.8 install hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 60\n",
      "drwxrwxr-x 3 ubuntu ubuntu  4096 Mar 27 16:07 pkg\n",
      "drwxr-xr-x 2 ubuntu ubuntu  4096 Apr 10 19:03 dis_materials\n",
      "-rw-rw-r-- 1 ubuntu ubuntu   225 Apr 11 20:13 spark-yarn.py\n",
      "drwxrwxr-x 9 ubuntu ubuntu  4096 Apr 12 10:46 python-spark-tutorial\n",
      "drwxrwxr-x 3 ubuntu ubuntu 36864 Apr 12 11:14 Project_Dataset\n",
      "-rwxrwxr-x 1 ubuntu ubuntu   143 Apr 19 06:37 jn.sh\n",
      "drwxrwxr-x 4 ubuntu ubuntu  4096 Apr 19 10:00 notebooks\n",
      "/home/ubuntu/.hdfscli.cfg\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr /home/ubuntu/\n",
    "\n",
    "!ls /home/ubuntu/.hdfscli.cfg\n",
    "\n",
    "!hadoop fs -ls /Data\n",
    "!hadoop fs -ls /Data/Binance_Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the hadoop hostname to read files and dir\n",
    "hdp_master = 'hdfs://master:9000'\n",
    "parquet_dir = '/Data/Binance_Parquet'\n",
    "session = SparkSession.builder.appName('DAT500').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import Config\n",
    "client = Config().get_client('dev')\n",
    "parquet_files = client.list(parquet_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/768 [00:02<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for parquet_file in tqdm.tqdm(parquet_files):\n",
    "    parquet_file = os.path.join(hdp_master,parquet_dir,parquet_file)\n",
    "    df = pq.read_table(f\"{hdp_master}/Data/Binance_Parquet/AE-BTC.parquet\")\n",
    "    df = df.to_pandas()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('First', 1), ('Second', 2), ('Third', 3), ('Fourth', 4), ('Fifth', 5)]\n",
    "df = session.createDataFrame(data)\n",
    "df.write.csv(f\"{hdp_master}/example.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "a = session.read.csv(f\"{hdp_master}/Abcnews-date-text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_load = session.read.csv(f\"{hdp_master}/example.csv\")\n",
    "df_load.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
